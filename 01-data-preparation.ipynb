{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from typing import Iterable, TypeVar\n",
    "from feature_engine.encoding import CountFrequencyEncoder\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading environment\n",
    "load_dotenv(verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Consider including the location of the dataset in you pc or any cloud storage service\n",
    "train_df = pd.read_csv(os.getenv('DATA_TRAIN'))\n",
    "test_df = pd.read_csv(os.getenv('DATA_TEST'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data manual processing\n",
    "---\n",
    "Before using the more sophisticated `PyCaret` processing tools. Let us do some manual data <span style=\"color:blue; ont-family:Courier; background-color:#FCE856; border-radius:10px; padding:3px;\">pre-pre-processing</span> üí°. The process is made up of the fllowing steps:\n",
    "\n",
    "<input type=\"checkbox\" checked> Data imputation, fixing and transformation\n",
    "\n",
    "<input type=\"checkbox\" checked> Feature engineering using `feature_engine`\n",
    "\n",
    "<input type=\"checkbox\" checked> Save final `train` and `test` datasets üëâüèæ *to step 2...*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data imputation, fixing and transformation\n",
    "---\n",
    "Now we try to extract more value from data by doing some operations over it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Based in my expertise as an Analyst, there're some fields I think aren't important for predicting the performance of the students. \n",
    "# Even because the information they provide is irrelevant or it's likely redundant, I chose to remove them.\n",
    "# However, there are other fields that may or may not be reundant, so I'll do a correlation analysis to see if its goot to get rid of them\n",
    "\n",
    "# Excluded fields: \n",
    "#   'cole_cod_dane_establecimiento', 'cole_cod_dane_sede', 'cole_codigo_icfes', 'cole_depto_ubicacion', 'cole_mcpio_ubicacion', \n",
    "#   'cole_nombre_establecimiento', 'cole_nombre_sede', 'estu_depto_presentacion', 'estu_depto_reside', 'estu_id', 'estu_mcpio_presentacion', \n",
    "#   'estu_mcpio_reside', 'estu_tipodocumento', 'periodo', 'cole_sede_principal', 'estu_cod_reside_depto', 'cole_naturaleza', \n",
    "#   'estu_cod_reside_mcpio', 'estu_nacionalidad', 'estu_pais_reside', 'estu_estadoinvestigacion', 'estu_estudiante'\n",
    "\n",
    "relevant_columns = [\n",
    "    'cole_area_ubicacion', 'cole_bilingue', 'cole_calendario', 'cole_caracter', 'cole_cod_depto_ubicacion', 'cole_cod_mcpio_ubicacion', \n",
    "    'estu_cod_depto_presentacion', 'estu_cod_mcpio_presentacion', 'cole_genero', 'cole_jornada', 'estu_fechanacimiento', 'estu_genero', \n",
    "    'estu_privado_libertad', 'fami_cuartoshogar', 'fami_educacionmadre', 'fami_educacionpadre', 'fami_estratovivienda', 'fami_personashogar', \n",
    "    'fami_tieneautomovil', 'fami_tienecomputador', 'fami_tieneinternet', 'fami_tienelavadora', 'target'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data imputation, fixing and trasformation functions\n",
    "# [DISCLAIMER] The method used to impute <cole_bilingue> isn't formal and is based ONLY in a quick analysis of the data\n",
    "mode_columns = [\n",
    "    'cole_caracter', 'fami_cuartoshogar', 'fami_educacionmadre', 'fami_educacionpadre', 'fami_estratovivienda', 'fami_personashogar',\n",
    "    'fami_tieneautomovil', 'fami_tienecomputador', 'fami_tieneinternet', 'fami_tienelavadora', 'estu_genero'\n",
    "]\n",
    "\n",
    "def data_mode_simple_imputation(df:pd.DataFrame, columns:Iterable[str]=mode_columns) -> pd.DataFrame:\n",
    "    for col in columns:\n",
    "        df.loc[:, col] = df.loc[:, col].fillna(value=df[col].mode().values[0])\n",
    "    \n",
    "    return df\n",
    "\n",
    "def data_bilingue_imputation(df:pd.DataFrame, column:str='cole_bilingue') -> pd.DataFrame:\n",
    "    df.loc[df[column].isna(), column] = df.apply(\n",
    "        lambda frame: 'S' if (\n",
    "            frame[column] in ('Estrato 4', 'Estrato 5', 'Estrato 6') and \n",
    "            (\n",
    "                frame.fami_tienecomputador == 'Si' or\n",
    "                frame.fami_tieneinternet == 'Si' or\n",
    "                frame.fami_tienelavadora == 'Si'\n",
    "            )\n",
    "        ) else 'N', axis=1\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_fecha_nacimiento_calculation(df:pd.DataFrame, column:str='estu_fechanacimiento') -> pd.DataFrame:\n",
    "    df.loc[:, column] = df[column].str.replace(pat=r'/000', repl='/200', regex=False)\n",
    "    df.loc[:, column] = pd.to_datetime(df[column], format='mixed', errors='coerce')\n",
    "\n",
    "    df.loc[:, column] = df[column].apply(\n",
    "        lambda x: np.round(\n",
    "            (pd.Timestamp('31/12/2023')-x)/pd.Timedelta(365, 'day'), 2\n",
    "        )\n",
    "    )\n",
    "\n",
    "    df = df.rename(columns={column:'estu_edad'})\n",
    "    df.loc[:, 'estu_edad'] = pd.to_numeric(df.estu_edad, errors='coerce')\n",
    "\n",
    "    return df\n",
    "\n",
    "def data_lower_transformation(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    string_columns = df.select_dtypes('object').columns.to_list()\n",
    "    df.loc[:, string_columns] = df.loc[:, string_columns].map(lambda x: x.lower() if type(x) == str else x)\n",
    "    df.loc[:, string_columns] = df.loc[:, string_columns].map(lambda x: 'si' if x == 's' else ('no' if x == 'n' else x)) \n",
    "\n",
    "    return df\n",
    "\n",
    "def data_get_dummies(df:pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    binary_columns = [column for column in df.columns if len(df[column].unique()) == 2 and column != 'target']\n",
    "    non_binary_target_columns = list(set(df.columns)-set(binary_columns))\n",
    "\n",
    "    df_1 = df.copy()[binary_columns]\n",
    "    df_1 = pd.get_dummies(df_1, drop_first=True, dtype=int)\n",
    "\n",
    "    df_2 = df.copy()[non_binary_target_columns]\n",
    "    \n",
    "    res = pd.concat([df_1, df_2], axis=1, ignore_index=False)\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train data processing\n",
    "train_df_relevant_columns = train_df[relevant_columns]\n",
    "train_df_relevant_columns = train_df_relevant_columns[train_df_relevant_columns.estu_genero.notna()]\n",
    "\n",
    "train_df_relevant_columns = (\n",
    "    train_df_relevant_columns\n",
    "        .pipe(data_mode_simple_imputation)\n",
    "        .pipe(data_bilingue_imputation)\n",
    "        .pipe(data_fecha_nacimiento_calculation)\n",
    "        .pipe(data_lower_transformation)\n",
    ")\n",
    "\n",
    "# Test data processing\n",
    "test_df_relevant_columns = test_df[['estu_id']+[elem for elem in relevant_columns if elem != 'target']]\n",
    "\n",
    "test_df_relevant_columns = (\n",
    "    test_df_relevant_columns\n",
    "        .pipe(data_mode_simple_imputation)\n",
    "        .pipe(data_bilingue_imputation)\n",
    "        .pipe(data_fecha_nacimiento_calculation)\n",
    "        .pipe(data_lower_transformation)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature engineering\n",
    "---\n",
    "After completing the initial data processing, we now perform some **Data Engineering** tasks to enhance their features. These techniques are primarily based on an initial **Data Encoding** process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_relevant_columns = train_df_relevant_columns.pipe(data_get_dummies)\n",
    "test_df_relevant_columns = test_df_relevant_columns.pipe(data_get_dummies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Columns without order\n",
    "columns_no_order_1 = ['cole_calendario', 'cole_jornada', 'cole_genero', 'cole_caracter']\n",
    "columns_no_order_2 = ['estu_cod_depto_presentacion', 'cole_cod_depto_ubicacion', 'estu_cod_mcpio_presentacion', 'cole_cod_mcpio_ubicacion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter('ignore')\n",
    "    \n",
    "    # For training data\n",
    "    cfenc_train = CountFrequencyEncoder(encoding_method='frequency')\n",
    "    cfenc_train.fit(train_df_relevant_columns[columns_no_order_1])\n",
    "    train_df_relevant_columns.loc[:, columns_no_order_1] = cfenc_train.transform(train_df_relevant_columns[columns_no_order_1])\n",
    "\n",
    "    scaler_train = MinMaxScaler()\n",
    "    scaler_train.fit(train_df_relevant_columns[columns_no_order_2])\n",
    "    train_df_relevant_columns.loc[:, columns_no_order_2] = pd.DataFrame(\n",
    "        data=scaler_train.transform(train_df_relevant_columns[columns_no_order_2]).astype('float'), columns=columns_no_order_2\n",
    "    )\n",
    "\n",
    "    # For test data\n",
    "    cfenc_test = CountFrequencyEncoder(encoding_method='frequency')\n",
    "    cfenc_test.fit(test_df_relevant_columns[columns_no_order_1])\n",
    "    test_df_relevant_columns.loc[:, columns_no_order_1] = cfenc_test.transform(test_df_relevant_columns[columns_no_order_1])\n",
    "\n",
    "    scaler_test = MinMaxScaler()\n",
    "    scaler_test.fit(test_df_relevant_columns[columns_no_order_2])\n",
    "    test_df_relevant_columns.loc[:, columns_no_order_2] = pd.DataFrame(\n",
    "        data=scaler_test.transform(test_df_relevant_columns[columns_no_order_2]).astype('float'), columns=columns_no_order_2\n",
    "    )\n",
    "\n",
    "# In case you'd like to save the tranied scalers\n",
    "# joblib.dump(cfenc_train, 'model/cfenc_train.gz')\n",
    "# joblib.dump(scaler_train, 'model/scaler_train.gz')\n",
    "\n",
    "# joblib.dump(cfenc_test, 'model/cfenc_test.gz')\n",
    "# joblib.dump(scaler_test, 'model/scaler_test.gz');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_columns_sorted = [\n",
    "    'cole_area_ubicacion_urbano','cole_bilingue_si','cole_calendario','cole_caracter','cole_cod_depto_ubicacion','cole_genero',\n",
    "    'cole_jornada','estu_cod_depto_presentacion','estu_edad','estu_genero_m','estu_privado_libertad_si','fami_cuartoshogar',\n",
    "    'fami_educacionmadre','fami_educacionpadre','fami_estratovivienda','fami_personashogar','fami_tieneautomovil_si','fami_tienecomputador_si',\n",
    "    'fami_tieneinternet_si','fami_tienelavadora_si','target'\n",
    "]\n",
    "\n",
    "train_df_relevant_columns = train_df_relevant_columns[train_columns_sorted]\n",
    "\n",
    "test_columns_sorted = [\n",
    "    'estu_id','cole_area_ubicacion_urbano','cole_bilingue_si','cole_calendario','cole_caracter','cole_cod_depto_ubicacion',\n",
    "    'cole_genero','cole_jornada','estu_cod_depto_presentacion','estu_edad','estu_genero_m','estu_privado_libertad_si',\n",
    "    'fami_cuartoshogar','fami_educacionmadre','fami_educacionpadre','fami_estratovivienda','fami_personashogar','fami_tieneautomovil_si',\n",
    "    'fami_tienecomputador_si','fami_tieneinternet_si','fami_tienelavadora_si'\n",
    "]\n",
    "\n",
    "test_df_relevant_columns = test_df_relevant_columns[test_columns_sorted]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving processed data\n",
    "train_df_relevant_columns.to_csv('data/train_df.csv', sep=';', index=False)\n",
    "test_df_relevant_columns.to_csv('data/test_df.csv', sep=';', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_class",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
